from typing import List
from conjecture.support.ExtractorBaseClass import ExtractorBaseClass
import torch


class QueryBasedDataExtraction(ExtractorBaseClass):

    def _query_model(
        self, prompt: str, model: torch.nn.Module, tokenizer: torch.nn.Module
    ) -> str:
        """
        Queries the model with a given prompt and returns the generated text.

        Args:
            prompt (str): The prompt to query the model with.
            model (torch.nn.Module): The model to use for generation.
            tokenizer (torch.nn.Module): The tokenizer to preprocess the prompt and decode the output.

        Returns:
            str: The generated text from the model.
        """
        inputs = tokenizer(prompt, return_tensors="pt")
        with torch.no_grad():
            outputs = model.generate(
                inputs["input_ids"], max_length=100, num_return_sequences=1
            )
        return tokenizer.decode(outputs[0], skip_special_tokens=True)

    def _check_string_in_text(self, target_string: str, generated_text: str) -> bool:
        """
        Checks if the target string is present in the generated text.

        Args:
            target_string (str): The string to look for in the generated text.
            generated_text (str): The text generated by the model.

        Returns:
            bool: True if the target string is found in the generated text, False otherwise.
        """
        return target_string.lower() in generated_text.lower()

    def estimate_likelihood(self, target_string: str, prompts: List[str]) -> float:
        """
        Estimates the likelihood of a target string being in the training data based on the model's responses.

        Args:
            target_string (str): The string to estimate the likelihood for.
            prompts (List[str]): A list of prompts to query the model with.

        Returns:
            float: The likelihood percentage of the target string being in the training data.
        """
        match_count = 0

        for prompt in prompts:
            generated_text = self._query_model(prompt, self._model, self._tokenizer)
            if self._check_string_in_text(target_string, generated_text):
                match_count += 1
                print(f"Match found in prompt: '{prompt}'")
                print(f"Generated Text: {generated_text}")

        # Calculate the likelihood as a percentage
        likelihood = (match_count / len(prompts)) * 100
        return likelihood
